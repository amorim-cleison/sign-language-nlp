debug: True
log: ../../work/sl-transformer/log
cuda: False
seed: 1

model: model.LSTM
# mode: train
mode: grid
resumable: False
workdir: ../../work/sl-transformer/checkpoint/{model}/{mode}

dataset_args:
  dataset_dir: ../../work/dataset/asl-phono/phonology/3d
  fields: [
    orientation_dh,
    orientation_ndh, 
    movement_dh, 
    movement_ndh,
    handshape_dh, 
    handshape_ndh, 
    # mouth_openness
  ]
  samples_min_freq: 2   # How many samples for the label should exist in dataset for the label to be considered?
  train_split_ratio: 0.7
  val_split_ratio: 0.1
  composition_strategy: as_words

model_args:
  input_size: 512
  hidden_size: 2048
  num_layers: 6
  dropout: 0.1
  # Transformer:
  # num_heads: 8


training_args:
  criterion: torch.nn.CrossEntropyLoss
  optimizer: torch.optim.SGD
  max_epochs: 100 # Original: 100
  batch_size: 50
  lr: 0.50
  early_stopping:
    patience: 20
  gradient_clipping:
    gradient_clip_value: 0.5
  lr_scheduler: 
    policy: ReduceLROnPlateau
    factor: 0.95
    patience: 2


grid_args:
  verbose: 3
  scoring: accuracy
  model_args:
    input_size: [128, 512, 1024]
    hidden_size: [128, 512, 2048]
    num_layers: [2, 10, 20]
    dropout: [0.1, 0.01]
  training_args:
    lr: [0.001, 0.01, 0.10]
    