debug: False
cuda: True
seed: 1
mode: grid
resumable: False
workdir: ../../work/sl-transformer/checkpoint/{model}/{mode}
scoring: [precision_macro, recall_macro, f1_macro, precision_weighted, recall_weighted, f1_weighted]
verbose: 3
n_jobs: -1


# minfreq   entries | srcvocab  tgtvocab  | input   hidden  layers  | loss (train)  f1 (weighted)
# 2         1245    | 19687     2286      | 2048*   2048    2       | 6.2719        0.0330          @15
# 2         1245    | 19687     2286      | 1024*   2048    2       | 6.3152        0.0330          @15
# 2         1245    | 19687     2286      | 1024*   2048    4*      | 6.3834        0.0330          @15
# 2         1245    | 19687     2286      | 8192*   2048    1*      | 6.2363        0.0313          @15
# 2         1245    | 19687     2286      | 4096*   4096*   2       | 6.2898        0.0313          @15
# 2         1245    | 19687     2286      | 2048*   2048    1*      | 6.2629        0.0312          @15
# 2         1245    | 19687     2286      | 16384*  2048    1*      | 6.2357        0.0303          @15
# 2         1245    | 19687     2286      | 2048*   4096*   2       | 6.2850        0.0303          @15
# 2         1245    | 19687     2286      | 2048*   4096*   4*      | 6.5378        0.0286          @15
# 2         1245    | 19687     2286      | 1024*   2048    1*      | 6.3189        0.0284          @15
# 2         1245    | 19687     2286      | 512*    2048    2       | 5.6445        0.0072          @10
# 2         1245    | 19687     2286      | 8192*   512*    4*      | 6.4630        0.0010          @15
# 2         1245    | 19687     2286      | 2048*   512*    4*      | 6.4636        0.0010          @15
# 2         1245    | 19687     2286      | 2048*   1024*   4*      | 6.4795        0.0010          @15

# 2         11232*  | 19687     2286      | 2048*   2048    2       | 7.3904        0.0154          @10 (early stop)
# 5*        5656*   | 10271*    670*      | 2048*   2048    2       | 6.2640        0.0116          @10 (early stop)

# 2         11232*  | 19687     2286      | 1024*   2048    2       | 7.4828        0.0073          @10 (early stop)
# 2         11232*  | 19687     2286      | 1024*   2048    4       | 7.5611        0.0012          @10 (early stop)

# Model:
model: model.LSTMNew
model_args:
  embedding_size: 1024
  hidden_size: 2048
  num_layers: 4
  dropout: 0.5
  # min_freq = 2
  #   len(src_vocab) = 19687
  #   len(tgt_vocab) = 2286
  #   entries(debug) = 1245
  #   entries(n-debug) = 11232
  # min_freq = 5
  #   len(src_vocab) = 10271
  #   len(tgt_vocab) = 670
  #   entries(debug) = 1231
  #   entries(n-debug) = 5656

# Criterion:
criterion: torch.nn.CrossEntropyLoss

# Optimizer:
optimizer: torch.optim.SGD
optimizer_args:
  # SGD ---
  nesterov: False
  momentum: 0.9
  # dampening: 0
  # weight_decay: 0

# Dataset:
dataset_args:
  dataset_dir: ../../work/dataset/asl-phono/phonology/3d
  fields: [
    orientation_dh,
    orientation_ndh, 
    movement_dh, 
    movement_ndh,
    handshape_dh, 
    handshape_ndh, 
    # mouth_openness
  ]
  samples_min_freq: 1   # How many samples for the label should exist in dataset for the label to be considered?
  composition_strategy: as_words
  reuse_transient: True
  balance_dataset: True

# Training:
training_args:
  max_epochs: 100 # Original: 100
  batch_size: 50
  lr: 0.1
  test_size: 0.15
  valid_size: 0.15
  early_stopping:
    patience: 10
    threshold: 1e-4
    threshold_mode: abs
  gradient_clipping:
    gradient_clip_value: 0.5
  lr_scheduler: 
    policy: ReduceLROnPlateau
    factor: 0.2
    patience: 5

# Grid search:
grid_args:
  cv: 5
  model_args:
    input_size: [1024, 8192, 16384]
    hidden_size: [2048, 16384, 32768]
    num_layers: [1, 2, 10]
  