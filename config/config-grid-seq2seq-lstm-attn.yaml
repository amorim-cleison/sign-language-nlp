debug: False
cuda: True
seed: 1
mode: grid
resumable: False
workdir: ../../work/sl-transformer/checkpoint/{model}/{mode}
scoring: [precision_macro, recall_macro, f1_macro, precision_weighted, recall_weighted, f1_weighted]
verbose: 3
n_jobs: -1


# minfreq   entries | srcvocab  tgtvocab  | embedding_size  hidden  layers  dropout | loss (train)  f1 (weighted)
# 2         12142*  | 19687     2286      | 1024            2048    2       0.5     | 0.2505        0.4567        @33
#                                                                                   > 0.0397        0.4867        @48
# 2         12142*  | 19687     2286      | 512             2048    2       0.5     | 0.0040        0.4524        @124

# (after fixing attention inputs)
# 2         12142*  | 19687     2286      | 1024            2048    2       0.1     | ?        @?


# Model:
model: model.Seq2SeqLSTMAttn
model_args:
  embedding_size: 1024
  hidden_size: 2048
  num_layers: 2
  dropout: 0.1
  # min_freq = 2
  #   len(src_vocab) = 19687
  #   len(tgt_vocab) = 2286
  #   entries(debug) = 1245
  #   entries(n-debug) = 11232
  # min_freq = 5
  #   len(src_vocab) = 10271
  #   len(tgt_vocab) = 670
  #   entries(debug) = 1231
  #   entries(n-debug) = 5656

# Criterion:
criterion: torch.nn.CrossEntropyLoss

# Optimizer:
optimizer: torch.optim.SGD
optimizer_args:
  # SGD ---
  nesterov: False
  momentum: 0.9
  # dampening: 0
  # weight_decay: 0

# Dataset:
dataset_args:
  dataset_dir: ../../work/dataset/asl-phono/phonology/3d
  fields: [
    orientation_dh,
    orientation_ndh, 
    movement_dh, 
    movement_ndh,
    handshape_dh, 
    handshape_ndh, 
    # mouth_openness
  ]
  samples_min_freq: 2   # How many samples for the label should exist in dataset for the label to be considered?
  composition_strategy: as_words
  reuse_transient: True
  balance_dataset: True

# Training:
training_args:
  max_epochs: 100 # Original: 100
  batch_size: 50
  lr: 0.1
  test_size: 0.15
  valid_size: 0.15
  early_stopping:
    patience: 20
    threshold: 1e-4
    threshold_mode: rel  # abs
  gradient_clipping:
    gradient_clip_value: 0.5
  lr_scheduler: 
    policy: ReduceLROnPlateau
    factor: 0.2
    patience: 5

# Grid search:
grid_args:
  cv: 5
  training_args:
    lr: 0.1
  model_args:
    input_size: [10, 50, 100]
    hidden_size: [50, 200, 500]
    num_layers: 2
  