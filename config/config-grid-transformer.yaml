debug: False
log: Null
cuda: True
seed: 1
mode: grid
resumable: False
workdir: ../../work/sl-transformer/checkpoint/{model}/{mode}

model: model.Transformer
# optimizer: torch.optim.Adam
optimizer: torch.optim.SGD
criterion: torch.nn.CrossEntropyLoss

dataset_args:
  dataset_dir: ../../work/dataset/asl-phono/phonology/3d
  fields: [
    orientation_dh,
    orientation_ndh, 
    movement_dh, 
    movement_ndh,
    handshape_dh, 
    handshape_ndh, 
    # mouth_openness
  ]
  samples_min_freq: 2   # How many samples for the label should exist in dataset for the label to be considered?
  composition_strategy: as_words

model_args:
  input_size: 512
  hidden_size: 2048
  num_layers: 6
  dropout: 0.1
  num_heads: 8

optimizer_args:
  # SGD ---
  nesterov: False
  # momentum: 0
  # dampening: 0
  # weight_decay: 0
  # ADAM ---
  # betas: [0.9, 0.98]
  # eps: 10e-9 

training_args:
  verbose: 3
  max_epochs: 100 # Original: 100
  batch_size: 50
  lr: 0.50
  early_stopping:
    monitor: valid_loss
    patience: 50
    lower_is_better: True
  train_split:
    cv: 0.1   # The proportion for validation
    stratified: False
  gradient_clipping:
    gradient_clip_value: 0.5
  lr_scheduler: 
    policy: ReduceLROnPlateau
    factor: 0.95
    patience: 1

grid_args:
  verbose: 3
  scoring: accuracy
  n_jobs: 1
  train_split:
    cv: 0.15
    stratified: False
  model_args:
    input_size: [64, 128, 256, 512]
    hidden_size: [128, 256, 512, 1024, 2048]
    num_layers: [2, 6, 10]
    dropout: [0.0, 0.1, 0.3]
    num_heads: [2, 4, 8, 16]
  training_args:
    lr: [0.1, 0.01, 0.001]
    