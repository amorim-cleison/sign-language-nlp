debug: False
cuda: True
seed: 1
mode: grid
resumable: False
workdir: '../../work/sl-transformer/checkpoint/{model}/{mode}'
scoring: accuracy
verbose: 3
n_jobs: -1

# Model:
model: model.Transformer
model_args:
  input_size: 512
  hidden_size: 2048
  num_layers: 6
  dropout: 0.1
  num_heads: 8

# Criterion:
criterion: torch.nn.CrossEntropyLoss

# Optimizer:
# optimizer: torch.optim.Adam
optimizer: torch.optim.SGD
optimizer_args:
  # SGD ---
  nesterov: False
  # momentum: 0
  # dampening: 0
  # weight_decay: 0
  # ADAM ---
  # betas: [0.9, 0.98]
  # eps: 10e-9 

# Dataset:
dataset_args:
  dataset_dir: ../../work/dataset/asl-phono/phonology/3d
  fields: [
    orientation_dh,
    orientation_ndh, 
    movement_dh, 
    movement_ndh,
    handshape_dh, 
    handshape_ndh, 
    # mouth_openness
  ]
  samples_min_freq: 5   # How many samples for the label should exist in dataset for the label to be considered?
  composition_strategy: as_words
  reuse_transient: False
  balance_dataset: True

# Training:
training_args:
  max_epochs: 10 # Original: 100
  batch_size: 50
  lr: 0.05
  test_size: 0.1
  valid_size: 0.2
  early_stopping:
    patience: 10
    threshold: 1e-4
    threshold_mode: abs
  gradient_clipping:
    gradient_clip_value: 0.5
  lr_scheduler: 
    policy: ReduceLROnPlateau
    factor: 0.95
    patience: 1

# Grid search:
grid_args:
  cv: 5
  training_args:
    batch_size: [50, 128]
    lr: [0.5, 0.05]
  # model_args:
  #   input_size: [64, 128, 256, 512]
  #   hidden_size: [128, 256, 512, 1024, 2048]
  #   num_layers: [2, 6, 10]
  #   dropout: [0.0, 0.1, 0.3]
  #   num_heads: [2, 4, 8, 16]
    